{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import datetime as dt\n",
    "import os.path\n",
    "import boto3\n",
    "import botocore\n",
    "import calendar\n",
    "import requests\n",
    "import moztelemetry.standards as moz_std\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_json(uri):\n",
    "    \"\"\" Perform an HTTP GET on the given uri, return the results as json.\n",
    "    If there is an error fetching the data, raise an exception.\n",
    "    \n",
    "    Args:\n",
    "        uri: the string URI to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        A JSON object with the response.\n",
    "    \"\"\"\n",
    "    data = requests.get(uri)\n",
    "    # Raise an exception if the fetch failed.\n",
    "    data.raise_for_status()\n",
    "    return data.json()\n",
    "\n",
    "def get_OS_arch(browser_arch, os_name, is_wow64):\n",
    "    \"\"\" Infers the OS arch from environment data.\n",
    "    \n",
    "    Args:\n",
    "        browser_arch: the browser architecture string (either \"x86\" or \"x86-64\").\n",
    "        os_name: the operating system name.\n",
    "        is_wow64: on Windows, indicates if the browser process is running under WOW64.\n",
    "    \n",
    "    Returns:\n",
    "        'x86' if the underlying OS is 32bit, 'x86-64' if it's a 64bit OS.\n",
    "    \"\"\"\n",
    "    \n",
    "    is_64bit_browser = browser_arch == 'x86-64'\n",
    "    # If it's a 64bit browser build, then we're on a 64bit system.\n",
    "    if is_64bit_browser:\n",
    "        return 'x86-64'\n",
    "    \n",
    "    is_windows = os_name == 'Windows_NT'\n",
    "    # If we're on Windows, with a 32bit browser build, and |isWow64 = true|,\n",
    "    # then we're on a 64 bit system.\n",
    "    if is_windows and is_wow64:\n",
    "        return 'x86-64'\n",
    "    \n",
    "    # Otherwise we're probably on a 32 bit system.\n",
    "    return 'x86'\n",
    "\n",
    "def vendor_name_from_id(id):\n",
    "    \"\"\" Get the string name matching the provided vendor id.\n",
    "    \n",
    "    Args:\n",
    "        id: A string containing the vendor id.\n",
    "    \n",
    "    Returns: \n",
    "        A string containing the vendor name or \"(Other <ID>)\" if\n",
    "        unknown.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: We need to make this an external resource for easier\n",
    "    # future updates.\n",
    "    vendor_map = {\n",
    "        '0x1013': 'Cirrus Logic',\n",
    "        '0x1002': 'AMD',\n",
    "        '0x8086': 'Intel',\n",
    "        '0x5333': 'S3 Graphics',\n",
    "        '0x1039': 'SIS',\n",
    "        '0x1106': 'VIA',\n",
    "        '0x10de': 'NVIDIA',\n",
    "        '0x102b': 'Matrox',\n",
    "        '0x15ad': 'VMWare',\n",
    "        '0x80ee': 'Oracle VirtualBox',\n",
    "        '0x1414': 'Microsoft Basic',\n",
    "    }\n",
    "    \n",
    "    return vendor_map.get(id, \"Other\")\n",
    "\n",
    "def get_device_family_chipset(vendor_id, device_id):\n",
    "    \"\"\" Get the family and chipset strings given the vendor and device ids.\n",
    "    \n",
    "    Args:\n",
    "        vendor_id: a string representing the vendor id (e.g. '0xabcd').\n",
    "        device_id: a string representing the device id (e.g. '0xbcde').\n",
    "    \n",
    "    Returns:\n",
    "        A string in the format \"Device Family Name-Chipset Name\".\n",
    "    \"\"\"\n",
    "    if not vendor_id in device_map:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if not device_id in device_map[vendor_id]:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    return \"-\".join(device_map[vendor_id][device_id])\n",
    "\n",
    "def invert_device_map(m):\n",
    "    \"\"\" Inverts a GPU device map fetched from the jrmuizel's Github repo. \n",
    "    The layout of the fetched GPU map layout is:\n",
    "        Vendor ID -> Device Family -> Chipset -> [Device IDs]\n",
    "    We should convert it to:\n",
    "        Vendor ID -> Device ID -> [Device Family, Chipset]\n",
    "    \"\"\"\n",
    "    device_id_map = {}\n",
    "    for vendor, u in m.iteritems():\n",
    "        device_id_map['0x' + vendor] = {}\n",
    "        for family, v in u.iteritems():\n",
    "            for chipset, ids in v.iteritems():\n",
    "                device_id_map['0x' + vendor].update({('0x' + gfx_id): [family, chipset] for gfx_id in ids})\n",
    "    return device_id_map\n",
    "\n",
    "def build_device_map():\n",
    "    \"\"\" This function builds a dictionary that will help us mapping vendor/device ids to a \n",
    "    human readable device family and chipset name.\"\"\"\n",
    "\n",
    "    intel_raw = fetch_json(\"https://github.com/jrmuizel/gpu-db/raw/master/intel.json\")\n",
    "    nvidia_raw = fetch_json(\"https://github.com/jrmuizel/gpu-db/raw/master/nvidia.json\")\n",
    "    amd_raw = fetch_json(\"https://github.com/jrmuizel/gpu-db/raw/master/amd.json\")\n",
    "    \n",
    "    device_map = {}\n",
    "    device_map.update(invert_device_map(intel_raw))\n",
    "    device_map.update(invert_device_map(nvidia_raw))\n",
    "    device_map.update(invert_device_map(amd_raw))\n",
    "\n",
    "    return device_map\n",
    "    \n",
    "device_map = build_device_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to query the longitudinal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reasons why the data for a client can be discarded.\n",
    "REASON_INACTIVE = \"inactive\"\n",
    "REASON_BROKEN_DATA = \"broken\"\n",
    "\n",
    "def get_valid_client_record(r, data_index):\n",
    "    \"\"\" Check if the referenced record is sane or contains partial/broken data.\n",
    "    \n",
    "    Args:\n",
    "        r: The client entry in the longitudinal dataset.\n",
    "        dat_index: The index of the sample within the client record.\n",
    "    \n",
    "    Returns:\n",
    "        An object containing the client hardware data or REASON_BROKEN_DATA if the\n",
    "        data is invalid.\n",
    "    \"\"\"\n",
    "    gfx_adapters = r[\"system_gfx\"][data_index][\"adapters\"]\n",
    "    monitors = r[\"system_gfx\"][data_index][\"monitors\"]\n",
    "    \n",
    "    # We should make sure to have GFX adapter. If we don't, discard this record.\n",
    "    if not gfx_adapters or not gfx_adapters[0]:\n",
    "        return REASON_BROKEN_DATA\n",
    "    \n",
    "    # Due to bug 1175005, Firefox on Linux isn't sending the screen resolution data.\n",
    "    # Don't discard the rest of the ping for that: just set the resolution to 0 if\n",
    "    # unavailable. See bug 1324014 for context.\n",
    "    screen_width = 0\n",
    "    screen_height = 0\n",
    "    if monitors and monitors[0]:\n",
    "        screen_width = monitors[0][\"screen_width\"]\n",
    "        screen_height = monitors[0][\"screen_height\"]\n",
    "    \n",
    "    # Non Windows OS do not have that property.\n",
    "    is_wow64 = r[\"system\"][data_index][\"is_wow64\"] == True\n",
    "    \n",
    "    # At this point, we should have filtered out all the weirdness. Fetch\n",
    "    # the data we need. \n",
    "    data = {\n",
    "        'browser_arch': r[\"build\"][data_index][\"architecture\"],\n",
    "        'os_name': r[\"system_os\"][data_index][\"name\"],\n",
    "        'os_version': r[\"system_os\"][data_index][\"version\"],\n",
    "        'memory_mb': r[\"system\"][data_index][\"memory_mb\"],\n",
    "        'is_wow64': is_wow64,\n",
    "        'gfx0_vendor_id': gfx_adapters[0][\"vendor_id\"],\n",
    "        'gfx0_device_id': gfx_adapters[0][\"device_id\"],\n",
    "        'screen_width': screen_width,\n",
    "        'screen_height': screen_height,\n",
    "        'cpu_cores': r[\"system_cpu\"][data_index][\"cores\"],\n",
    "        'cpu_vendor': r[\"system_cpu\"][data_index][\"vendor\"],\n",
    "        'cpu_speed': r[\"system_cpu\"][data_index][\"speed_mhz\"],\n",
    "        'has_flash': False\n",
    "    }\n",
    "    \n",
    "    # The plugins data can still be null or empty, account for that.\n",
    "    plugins = r[\"active_plugins\"][data_index] if r[\"active_plugins\"] else None\n",
    "    if plugins:\n",
    "        data['has_flash'] = any([True for p in plugins if p['name'] == 'Shockwave Flash'])\n",
    "    \n",
    "    return REASON_BROKEN_DATA if None in data.values() else data\n",
    "\n",
    "def get_latest_valid_per_client(entry, time_start, time_end):\n",
    "    \"\"\" Get the most recently submitted ping for a client within the given timeframe.\n",
    "\n",
    "    Then use this index to look up the data from the other columns (we can assume that the sizes\n",
    "    of these arrays match, otherwise the longitudinal dataset is broken).\n",
    "    Once we have the data, we make sure it's valid and return it.\n",
    "    \n",
    "    Args:\n",
    "        entry: The record containing all the data for a single client.\n",
    "        time_start: The beginning of the reference timeframe.\n",
    "        time_end: The end of the reference timeframe.\n",
    "\n",
    "    Returns:\n",
    "        An object containing the valid hardware data for the client or a string\n",
    "        describing why the data is discarded. Either REASON_INACTIVE, if the client didn't\n",
    "        submit a ping within the desired timeframe, or REASON_BROKEN_DATA if it send\n",
    "        broken data. \n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if the columns within the record have mismatching lengths. This\n",
    "        means the longitudinal dataset is corrupted.\n",
    "    \"\"\"\n",
    "    latest_entry = None\n",
    "    for index, pkt_date in enumerate(entry[\"submission_date\"]):\n",
    "        sub_date = dt.datetime.strptime(pkt_date, \"%Y-%m-%dT%H:%M:%S.%fZ\").date()\n",
    "        # The data is in descending order, the most recent ping comes first.\n",
    "        # The first item less or equal than the time_end date is our thing.\n",
    "        if sub_date >= time_start and sub_date <= time_end:\n",
    "            latest_entry = index\n",
    "            break\n",
    "        \n",
    "        # Ok, we went too far, we're not really interested in the data\n",
    "        # outside of [time_start, time_end]. Since records are ordered,\n",
    "        # we can actually skip this.\n",
    "        if sub_date < time_start:\n",
    "            break\n",
    "    \n",
    "    # This client wasn't active in the reference timeframe, just map it to no data.\n",
    "    if latest_entry is None:\n",
    "        return REASON_INACTIVE\n",
    "\n",
    "    # Some clients might be missing entire sections. If that's\n",
    "    # a basic section, skip them, we don't want partial data.\n",
    "    # Don't enforce the presence of \"active_plugins\", as it's not included\n",
    "    # by the pipeline if no plugin is reported by Firefox (see bug 1333806).\n",
    "    desired_sections = [\n",
    "        \"build\", \"system_os\", \"submission_date\", \"system\",\n",
    "        \"system_gfx\", \"system_cpu\"\n",
    "    ]\n",
    "\n",
    "    for field in desired_sections:\n",
    "        if entry[field] is None:\n",
    "            return REASON_BROKEN_DATA\n",
    "\n",
    "        # All arrays in the longitudinal dataset should have the same length, for a\n",
    "        # single client. If that's not the case, if our index is not there, throw.\n",
    "        if entry[field][latest_entry] is None:\n",
    "            raise ValueError(\"Null \" + field + \" index: \" + str(latest_entry))\n",
    "\n",
    "    return get_valid_client_record(entry, latest_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how we transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(p):\n",
    "    \"\"\" This function prepares the data for further analyses (e.g. unit conversion,\n",
    "    vendor id to string, ...). \"\"\"\n",
    "    cpu_speed = round(p['cpu_speed'] / 1000.0, 1)\n",
    "    return {\n",
    "        'browser_arch': p['browser_arch'],\n",
    "        'cpu_cores': p['cpu_cores'],\n",
    "        'cpu_cores_speed': str(p['cpu_cores']) + '_' + str(cpu_speed),\n",
    "        'cpu_vendor': p['cpu_vendor'],\n",
    "        'cpu_speed': cpu_speed,\n",
    "        'gfx0_vendor_name': vendor_name_from_id(p['gfx0_vendor_id']),\n",
    "        'gfx0_model': get_device_family_chipset(p['gfx0_vendor_id'], p['gfx0_device_id']),\n",
    "        'resolution': str(p['screen_width']) + 'x' + str(p['screen_height']),\n",
    "        'memory_gb': int(round(p['memory_mb'] / 1024.0)),\n",
    "        'os': p['os_name'] + '-' + p['os_version'],\n",
    "        'os_arch': get_OS_arch(p['browser_arch'], p['os_name'], p['is_wow64']),\n",
    "        'has_flash': p['has_flash']\n",
    "    }\n",
    "\n",
    "def aggregate_data(processed_data):\n",
    "    def seq(acc, v):\n",
    "        # The dimensions over which we want to aggregate the different values.\n",
    "        keys_to_aggregate = [\n",
    "            'browser_arch',\n",
    "            'cpu_cores',\n",
    "            'cpu_cores_speed',\n",
    "            'cpu_vendor',\n",
    "            'cpu_speed',\n",
    "            'gfx0_vendor_name',\n",
    "            'gfx0_model',\n",
    "            'resolution',\n",
    "            'memory_gb',\n",
    "            'os',\n",
    "            'os_arch',\n",
    "            'has_flash'\n",
    "        ]\n",
    "\n",
    "        for key_name in keys_to_aggregate:\n",
    "            # We want to know how many users have a particular configuration (e.g. using a particular\n",
    "            # cpu vendor). For each dimension of interest, build a key as (hw, value) and count its\n",
    "            # occurrences among the user base.\n",
    "            acc_key = (key_name, v[key_name])\n",
    "            acc[acc_key] = acc.get(acc_key, 0) + 1\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def cmb(v1, v2):\n",
    "        # Combine the counts from the two partial dictionaries. Hacky?\n",
    "        return  { k: v1.get(k, 0) + v2.get(k, 0) for k in set(v1) | set(v2) }\n",
    "    \n",
    "    return processed_data.aggregate({}, seq, cmb)\n",
    "\n",
    "def collapse_buckets(aggregated_data, count_threshold):\n",
    "    \"\"\" Collapse uncommon configurations in generic groups to preserve privacy.\n",
    "    \n",
    "    This takes the dictionary of aggregated results from |aggregate_data| and collapses\n",
    "    entries with a value less than |count_threshold| in a generic bucket.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_data: The object containing aggregated data.\n",
    "        count_threhold: Groups (or \"configurations\") containing less than this value\n",
    "        are collapsed in a generic bucket.\n",
    "    \"\"\"\n",
    "    \n",
    "    # These fields have a fixed set of values and we need to report all of them.\n",
    "    EXCLUSION_LIST = [ \"has_flash\", \"browser_arch\", \"os_arch\" ]\n",
    "    \n",
    "    collapsed_groups = {}\n",
    "    for k,v in aggregated_data.iteritems():\n",
    "        key_type = k[0]\n",
    "        \n",
    "        # If the resolution is 0x0 (see bug 1324014), put that into the \"Other\"\n",
    "        # bucket.\n",
    "        if key_type == 'resolution' and k[1] == '0x0':\n",
    "            other_key = ('resolution', 'Other')\n",
    "            collapsed_groups[other_key] = collapsed_groups.get(other_key, 0) + v\n",
    "            continue\n",
    "    \n",
    "        # Don't clump this group into the \"Other\" bucket if it has enough\n",
    "        # users it in.\n",
    "        if v > count_threshold or key_type in EXCLUSION_LIST:\n",
    "            collapsed_groups[k] = v\n",
    "            continue\n",
    "        \n",
    "        # If we're here, it means that the key has not enough elements.\n",
    "        # Fall through the next cases and try to group things together.\n",
    "        new_group_key = 'Other'\n",
    "        \n",
    "        # Let's try to group similar resolutions together.\n",
    "        if key_type == 'resolution':\n",
    "            # Extract the resolution.\n",
    "            [w, h] = k[1].split('x')\n",
    "            # Round to the nearest hundred.\n",
    "            w = int(round(int(w), -2))\n",
    "            h = int(round(int(h), -2))\n",
    "            # Build up a new key.\n",
    "            new_group_key = '~' + str(w) + 'x' + str(h)\n",
    "        elif key_type == 'os':\n",
    "            [os, ver] = k[1].split('-', 1)\n",
    "            new_group_key = os + '-' + 'Other'\n",
    "        \n",
    "        # We don't have enough data for this particular group/configuration.\n",
    "        # Aggregate it with the data in the \"Other\" bucket\n",
    "        other_key = (k[0], new_group_key)\n",
    "        collapsed_groups[other_key] = collapsed_groups.get(other_key, 0) + v\n",
    "    \n",
    "    # The previous grouping might have created additional groups. Let's check again.\n",
    "    final_groups = {}\n",
    "    for k,v in collapsed_groups.iteritems():\n",
    "        # Don't clump this group into the \"Other\" bucket if it has enough\n",
    "        # users it in.\n",
    "        if (v > count_threshold and k[1] != 'Other') or k[0] in EXCLUSION_LIST:\n",
    "            final_groups[k] = v\n",
    "            continue\n",
    "\n",
    "        # We don't have enough data for this particular group/configuration.\n",
    "        # Aggregate it with the data in the \"Other\" bucket\n",
    "        other_key = (k[0], 'Other')\n",
    "        final_groups[other_key] = final_groups.get(other_key, 0) + v\n",
    "    \n",
    "    return final_groups\n",
    "\n",
    "def finalize_data(data, sample_count, broken_ratio, inactive_ratio, report_date):\n",
    "    \"\"\" Finalize the aggregated data.\n",
    "    \n",
    "    Translate raw sample numbers to percentages and add the date for the reported\n",
    "    week along with the percentage of discarded samples due to broken data.\n",
    "    \n",
    "    Rename the keys to more human friendly names.\n",
    "    \n",
    "    Args:\n",
    "        data: Data in aggregated form.\n",
    "        sample_count: The number of samples the aggregates where generated from.\n",
    "        broken_ratio: The percentage of samples discarded due to broken data.\n",
    "        inactive_ratio: The percentage of samples discarded due to the client not sending data.\n",
    "        report_date: The starting day for the reported week.\n",
    "    \n",
    "    Returns:\n",
    "        An object containing the reported hardware statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    denom = float(sample_count)\n",
    "\n",
    "    aggregated_percentages = {\n",
    "        'date': report_date.isoformat(),\n",
    "        'broken': broken_ratio,\n",
    "        'inactive': inactive_ratio,\n",
    "    }\n",
    "\n",
    "    keys_translation = {\n",
    "        'browser_arch': 'browserArch_',\n",
    "        'cpu_cores': 'cpuCores_',\n",
    "        'cpu_cores_speed': 'cpuCoresSpeed_',\n",
    "        'cpu_vendor': 'cpuVendor_',\n",
    "        'cpu_speed': 'cpuSpeed_',\n",
    "        'gfx0_vendor_name': 'gpuVendor_',\n",
    "        'gfx0_model': 'gpuModel_',\n",
    "        'resolution': 'resolution_',\n",
    "        'memory_gb': 'ram_',\n",
    "        'os': 'osName_',\n",
    "        'os_arch': 'osArch_',\n",
    "        'has_flash': 'hasFlash_'\n",
    "    }\n",
    "\n",
    "    # Compute the percentages from the raw numbers.\n",
    "    for k, v in data.iteritems():\n",
    "        # The old key is a tuple (key, value). We translate the key part and concatenate the\n",
    "        # value as a string.\n",
    "        new_key = keys_translation[k[0]] + unicode(k[1])\n",
    "        aggregated_percentages[new_key] = v / denom\n",
    "\n",
    "    return aggregated_percentages\n",
    "\n",
    "def validate_finalized_data(data):\n",
    "    \"\"\" Validate the aggregated and finalized data.\n",
    "    \n",
    "    This checks that the aggregated hardware data object contains all the expectd keys\n",
    "    and that they sum up roughly 1.0.\n",
    "    \n",
    "    When a problem is found a message is printed to make debugging easier.\n",
    "    \n",
    "    Args:\n",
    "        data: Data in aggregated form.\n",
    "    \n",
    "    Returns:\n",
    "        True if the data validates correctly, false otherwise.\n",
    "    \"\"\"\n",
    "    keys_accumulator = {\n",
    "        'browserArch': 0.0,\n",
    "        'cpuCores': 0.0,\n",
    "        'cpuCoresSpeed': 0.0,\n",
    "        'cpuVendor': 0.0,\n",
    "        'cpuSpeed': 0.0,\n",
    "        'gpuVendor': 0.0,\n",
    "        'gpuModel': 0.0,\n",
    "        'resolution': 0.0,\n",
    "        'ram': 0.0,\n",
    "        'osName': 0.0,\n",
    "        'osArch': 0.0,\n",
    "        'hasFlash': 0.0\n",
    "    }\n",
    "    \n",
    "    # We expect to have at least a key in |data| whose name begins with one\n",
    "    # of the keys in |keys_accumulator|. Iterate through the keys in |data|\n",
    "    # and accumulate their values in the accumulator.\n",
    "    for key, value in data.iteritems():\n",
    "        if key in [\"inactive\", \"broken\", \"date\"]:\n",
    "            continue\n",
    "        \n",
    "        # Get the name of the property to look it up in the accumulator.\n",
    "        property_name = key.split('_')[0]\n",
    "        if property_name not in keys_accumulator:\n",
    "            print(\"Cannot find {} in |keys_accumulator|\".format(property_name))\n",
    "            return False\n",
    "        \n",
    "        keys_accumulator[property_name] += value\n",
    "\n",
    "    # Make sure all the properties add up to 1.0 (or close enough).\n",
    "    for key, value in keys_accumulator.iteritems():\n",
    "        if abs(1.0 - value) > 0.05:\n",
    "            print(\"{} values do not add up to 1.0. Their sum is {}.\".format(key, value))\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File and S3 serialization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S3_PUBLIC_BUCKET = \"telemetry-public-analysis-2\"\n",
    "S3_DATA_PATH = \"game-hardware-survey/data/\"\n",
    "\n",
    "def get_file_name(suffix=\"\"):\n",
    "    return \"hwsurvey-weekly\" + suffix + \".json\"\n",
    "\n",
    "def serialize_results(aggregated_data, week_start, week_end):\n",
    "    # Write the week start/end in the filename.\n",
    "    suffix = \"-\" + week_start.strftime(\"%Y%d%m\") + \"-\" + week_end.strftime(\"%Y%d%m\")\n",
    "    file_name = get_file_name(suffix)\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print \"{} exists, we will overwrite it.\".format(file_name)\n",
    "\n",
    "    # Our aggregated data is a JSON object.\n",
    "    json_entry = json.dumps(aggregated_data)\n",
    "\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(\"[\" + json_entry.encode('utf8') + \"]\\n\")\n",
    "\n",
    "def fetch_previous_state(s3_source_file_name, local_file_name):\n",
    "    \"\"\"\n",
    "    This function fetches the previous state from S3's bucket and stores it locally.\n",
    "    \n",
    "    Args:\n",
    "        s3_source_file_name: The name of the file on S3.\n",
    "        local_file_name: The name of the file to save to, locally.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the previous state.\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "    key_path = S3_DATA_PATH + s3_source_file_name\n",
    "    \n",
    "    try:\n",
    "        transfer.download_file(S3_PUBLIC_BUCKET, key_path, local_file_name)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # If the file wasn't there, that's ok. Otherwise, abort!\n",
    "        if e.response['Error']['Code'] != \"404\":\n",
    "            raise e\n",
    "        else:\n",
    "            print \"Did not find an existing file at '{}'\".format(key_path)\n",
    "\n",
    "def store_new_state(source_file_name, s3_dest_file_name):\n",
    "    \"\"\"\n",
    "    Store the new state file to S3.\n",
    "    \n",
    "    Args:\n",
    "        source_file_name: The name of the local source file.\n",
    "        s3_dest_file_name: The name of the destination file on S3.\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client('s3', 'us-west-2')\n",
    "    transfer = boto3.s3.transfer.S3Transfer(client)\n",
    "    \n",
    "    # Update the state in the analysis bucket.\n",
    "    key_path = S3_DATA_PATH + s3_dest_file_name\n",
    "    transfer.upload_file(source_file_name, S3_PUBLIC_BUCKET, key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main logic, wiring all the things together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_report(start_date=None, end_date=None):\n",
    "    \"\"\" Generates the hardware survey dataset for the reference timeframe.\n",
    "    \n",
    "    If the timeframe is longer than a week, split it in in weekly chunks\n",
    "    and process each chunk individually (eases backfilling).\n",
    "    \n",
    "    The report for each week is saved in a local JSON file.\n",
    "    \n",
    "    Args:\n",
    "        start_date: The date from which we start generating the report. If None,\n",
    "           the report starts from the beginning of the past week (Sunday).\n",
    "        end_date: The date the marks the end of the reporting period. This only\n",
    "           makes sense if a |start_date| was provided. If None, this defaults\n",
    "           to the end of the past week (Saturday).\n",
    "    \"\"\"\n",
    "\n",
    "    # If no start_date was provided, generate a report for the past complete week.\n",
    "    last_week = moz_std.get_last_week_range()\n",
    "    date_range = (\n",
    "        moz_std.snap_to_beginning_of_week(start_date, \"Sunday\") if start_date != None else last_week[0],\n",
    "        end_date if (end_date != None and start_date != None) else last_week[1]\n",
    "    )\n",
    "\n",
    "    # Connect to the longitudinal dataset.\n",
    "    sqlQuery = \"SELECT \" +\\\n",
    "               \"build,\" +\\\n",
    "               \"client_id,\" +\\\n",
    "               \"active_plugins,\" +\\\n",
    "               \"system_os,\" +\\\n",
    "               \"submission_date,\" +\\\n",
    "               \"system,\" +\\\n",
    "               \"system_gfx,\" +\\\n",
    "               \"system_cpu,\" +\\\n",
    "               \"normalized_channel \" +\\\n",
    "               \"FROM longitudinal\"\n",
    "    frame = sqlContext.sql(sqlQuery)\\\n",
    "                      .where(\"normalized_channel = 'release'\")\\\n",
    "                      .where(\"build is not null and build[0].application_name = 'Firefox'\")\n",
    "\n",
    "        \n",
    "    # The number of all the fetched records (including inactive and broken).\n",
    "    records_count = frame.count()\n",
    "    print \"Total record count: {}\".format(records_count)\n",
    "\n",
    "    # Split the submission period in chunks, so we don't run out of resources while aggregating if\n",
    "    # we want to backfill.\n",
    "    chunk_start = date_range[0]\n",
    "    chunk_end = None\n",
    "\n",
    "    while chunk_start < date_range[1]:\n",
    "        chunk_end = chunk_start + dt.timedelta(days=6)\n",
    "\n",
    "        # Fetch the data we need.\n",
    "        data = frame.rdd.map(lambda r: get_latest_valid_per_client(r, chunk_start, chunk_end))\n",
    "        \n",
    "        # Filter out broken data.\n",
    "        filtered_data = data.filter(lambda r: r not in [REASON_BROKEN_DATA, REASON_INACTIVE])\n",
    "        \n",
    "        # Count the broken records and inactive records.\n",
    "        discarded = data.filter(lambda r: r in [REASON_BROKEN_DATA, REASON_INACTIVE]).countByValue()\n",
    "        broken_count = discarded[REASON_BROKEN_DATA]\n",
    "        inactive_count = discarded[REASON_INACTIVE]\n",
    "        broken_ratio = broken_count / float(records_count)\n",
    "        inactive_ratio = inactive_count / float(records_count)\n",
    "        print \"Broken pings ratio: {}\\nInactive clients ratio: {}\".format(broken_ratio, inactive_ratio)\n",
    "        \n",
    "        # If we're not seeing sane values for the broken or inactive ratios,\n",
    "        # bail out early on. There's no point in aggregating.\n",
    "        if broken_ratio >= 0.9 or inactive_ratio >= 0.9:\n",
    "            raise Exception(\"Unexpected ratio of broken pings or inactive clients.\")\n",
    "        \n",
    "        # Process the data, transforming it in the form we desire.\n",
    "        processed_data = filtered_data.map(prepare_data)\n",
    "        \n",
    "        print \"Aggregating entries...\"\n",
    "        aggregated_pings = aggregate_data(processed_data)\n",
    "\n",
    "        # Get the sample count, we need it to compute the percentages instead of raw numbers.\n",
    "        # Since we're getting only the newest ping for each client, we can simply count the\n",
    "        # number of pings. THIS MAY NOT BE CONSTANT ACROSS WEEKS!\n",
    "        valid_records_count = filtered_data.count()\n",
    "\n",
    "        # Collapse together groups that count less than 1% of our samples.\n",
    "        threshold_to_collapse = int(valid_records_count * 0.01)\n",
    "        \n",
    "        print \"Collapsing smaller groups into the other bucket (threshold {th})\".format(th=threshold_to_collapse)\n",
    "        collapsed_aggregates = collapse_buckets(aggregated_pings, threshold_to_collapse)\n",
    "        \n",
    "        print \"Post-processing raw values...\"\n",
    "        \n",
    "        processed_aggregates = finalize_data(collapsed_aggregates,\n",
    "                                             valid_records_count,\n",
    "                                             broken_ratio,\n",
    "                                             inactive_ratio,\n",
    "                                             chunk_start)\n",
    "        \n",
    "        if not validate_finalized_data(processed_aggregates):\n",
    "            raise Exception(\"The aggregates failed to validate.\")\n",
    "        \n",
    "        print \"Serializing results locally...\"\n",
    "        # This either appends to an existing file, or creates a new one.\n",
    "        serialize_results(processed_aggregates, chunk_start, chunk_end)\n",
    "\n",
    "        # Move on to the next chunk, just add one day the end of the last chunk.\n",
    "        chunk_start = chunk_end + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the hardware report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = None # Only use this when backfilling, e.g. dt.date(2016,2,1)\n",
    "end_date = None # Only use this when backfilling, e.g. dt.date(2016,3,26)\n",
    "\n",
    "# Generate the report for the desired period.\n",
    "generate_report(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch the files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch the previous data from S3 and save it locally.\n",
    "fetch_previous_state(\"hwsurvey-weekly.json\", \"hwsurvey-weekly-prev.json\")\n",
    "# Concat the json files into the output.\n",
    "print \"Joining JSON files...\"\n",
    "!jq -s \"[.[]|.[]]\" *.json > \"hwsurvey-weekly.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and upload the produced output\n",
    "Attempt to read the file we're going to upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"hwsurvey-weekly.json\", \"rt\") as report_json:\n",
    "    # If we attempt to load invalid JSON from the assembled file,\n",
    "    # the next function throws.\n",
    "    json.load(report_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't fail so far, so upload our data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the new state to S3. Since S3 doesn't support symlinks, make two copy\n",
    "# of the file: one will always contain the latest data, the other for archiving.\n",
    "archived_file_copy = \"hwsurvey-weekly-\" + datetime.date.today().strftime(\"%Y%d%m\") + \".json\"\n",
    "store_new_state(\"hwsurvey-weekly.json\", archived_file_copy)\n",
    "store_new_state(\"hwsurvey-weekly.json\", \"hwsurvey-weekly.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds a bit of test coverage for the newly introduced functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_prepare_data():\n",
    "    data = {\n",
    "        'browser_arch': 'x86',\n",
    "        'os_name': 'Windows_NT',\n",
    "        'os_version': '6.1',\n",
    "        'memory_mb': 4286,\n",
    "        'is_wow64': False,\n",
    "        'gfx0_vendor_id': '0xfeee',\n",
    "        'gfx0_device_id': '0xd1d1',\n",
    "        'screen_width': 1280,\n",
    "        'screen_height': 1024,\n",
    "        'cpu_cores': 2,\n",
    "        'cpu_vendor': 'SomeCpuVendor',\n",
    "        'cpu_speed': 3261,\n",
    "        'has_flash': True\n",
    "    }\n",
    "    \n",
    "    prepared_data = prepare_data(data)\n",
    "    assert prepared_data[\"browser_arch\"] == \"x86\",\\\n",
    "           \"The browser architecture must be correct.\"\n",
    "    assert prepared_data[\"cpu_cores\"] == 2,\\\n",
    "           \"The number of CPU cores must be correct.\"\n",
    "    assert prepared_data[\"cpu_speed\"] == 3.3,\\\n",
    "           \"The CPU speed must be in GHz and correctly rounded to 1 decimal.\"\n",
    "    assert prepared_data[\"cpu_vendor\"] == \"SomeCpuVendor\",\\\n",
    "           \"The CPU vendor must be correct.\"\n",
    "    assert prepared_data[\"cpu_cores_speed\"] == \"2_3.3\",\\\n",
    "           \"The CPU cores and speed must be correctly merged together.\"\n",
    "    assert prepared_data[\"gfx0_vendor_name\"] == \"Other\",\\\n",
    "           \"The GPU vendor name must be correctly converted from the vendor id.\"\n",
    "    assert prepared_data[\"gfx0_model\"] == \"family-chipset\",\\\n",
    "           \"The GPU family and chipset must be correctly derived from the vendor and device ids.\"\n",
    "    assert prepared_data[\"resolution\"] == \"1280x1024\",\\\n",
    "           \"The screen resolution must be correctly concatenated.\"\n",
    "    assert prepared_data[\"memory_gb\"] == 4,\\\n",
    "           \"The RAM memory must be converted to GB.\"\n",
    "    assert prepared_data[\"os\"] == \"Windows_NT-6.1\",\\\n",
    "           \"The OS string must contain the OS name and version.\"\n",
    "    assert prepared_data[\"os_arch\"] == \"x86\",\\\n",
    "           \"The OS architecture must be correctly inferred.\"\n",
    "    assert prepared_data[\"has_flash\"] == True,\\\n",
    "           \"The flash plugin must be correctly reported.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the test coverage for the aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_aggregate_data():\n",
    "    raw_data = [\n",
    "        {\n",
    "            'browser_arch': 'x86',\n",
    "            'os_name': 'Windows_NT',\n",
    "            'os_version': '6.1',\n",
    "            'memory_mb': 4286,\n",
    "            'is_wow64': False,\n",
    "            'gfx0_vendor_id': '0xfeee',\n",
    "            'gfx0_device_id': '0xd1d1',\n",
    "            'screen_width': 1280,\n",
    "            'screen_height': 1024,\n",
    "            'cpu_cores': 2,\n",
    "            'cpu_vendor': 'SomeCpuVendor',\n",
    "            'cpu_speed': 3261,\n",
    "            'has_flash': True\n",
    "        },\n",
    "        {\n",
    "            'browser_arch': 'x86',\n",
    "            'os_name': 'Windows_NT',\n",
    "            'os_version': '6.1',\n",
    "            'memory_mb': 4286,\n",
    "            'is_wow64': False,\n",
    "            'gfx0_vendor_id': '0xfeee',\n",
    "            'gfx0_device_id': '0xd1d1',\n",
    "            'screen_width': 1280,\n",
    "            'screen_height': 1024,\n",
    "            'cpu_cores': 2,\n",
    "            'cpu_vendor': 'SomeCpuVendor',\n",
    "            'cpu_speed': 3261,\n",
    "            'has_flash': True\n",
    "        },\n",
    "        {\n",
    "            'browser_arch': 'x86-64',\n",
    "            'os_name': 'Darwin',\n",
    "            'os_version': '15.3',\n",
    "            'memory_mb': 8322,\n",
    "            'is_wow64': False,\n",
    "            'gfx0_vendor_id': '0xfeee',\n",
    "            'gfx0_device_id': '0xd1d2',\n",
    "            'screen_width': 1320,\n",
    "            'screen_height': 798,\n",
    "            'cpu_cores': 4,\n",
    "            'cpu_vendor': 'SomeCpuVendor',\n",
    "            'cpu_speed': 4211,\n",
    "            'has_flash': True\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Creat an rdd with the pepared data, then aggregate.\n",
    "    data_rdd = sc.parallelize([prepare_data(d) for d in raw_data])\n",
    "    agg_data = aggregate_data(data_rdd)\n",
    "    \n",
    "    assert agg_data[('os_arch', 'x86')] == 2,\\\n",
    "           \"Two 'x86' OS architectures must be reported.\"\n",
    "    assert agg_data[('os_arch', 'x86-64')] == 1,\\\n",
    "           \"One 'x86-64' OS architecture must be reported.\"\n",
    "    assert agg_data[('has_flash', True)] == 3,\\\n",
    "           \"All the entries had the flash plugin, so this must be 3.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the test coverage for the collapsing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_collapse_buckets():\n",
    "    agg_data = {\n",
    "        (\"has_flash\", True): 72,\n",
    "        (\"has_flash\", False): 2,\n",
    "        (\"resolution\", \"1280x1024\"): 50,\n",
    "        (\"resolution\", \"2560x1440\"): 8,\n",
    "        (\"resolution\", \"2563x1440\"): 8,\n",
    "        (\"resolution\", \"640x480\"): 6,\n",
    "        (\"resolution\", \"640x472\"): 2,\n",
    "        (\"resolution\", \"0x0\"): 15, # Invalid data.\n",
    "        (\"os\", \"Windows_NT-6.11\"): 34,\n",
    "        (\"os\", \"Windows_NT-5.10\"): 8,\n",
    "        (\"os\", \"Windows_NT-4\"): 8,\n",
    "        (\"os\", \"FunkyOS-4\"): 1,\n",
    "        (\"os\", \"Darwin-11.0\"): 22,\n",
    "        (\"os\", \"Darwin-1.0\"): 1,\n",
    "    }\n",
    "    \n",
    "    threshold = 10\n",
    "    collapsed_data = collapse_buckets(agg_data, threshold)\n",
    "    \n",
    "    # Test that keys with values above the threshold are kept.\n",
    "    assert (\"os\", \"Darwin-11.0\") in collapsed_data,\\\n",
    "           \"Keys with enough elements must not be collapsed.\"\n",
    "    assert (\"has_flash\", True) in collapsed_data,\\\n",
    "           \"Keys with enough elements must not be collapsed.\"\n",
    "    assert (\"resolution\", \"1280x1024\") in collapsed_data,\\\n",
    "           \"Keys with enough elements must not be collapsed.\"\n",
    "    assert (\"os\", \"Windows_NT-6.11\") in collapsed_data,\\\n",
    "           \"Keys with enough elements must not be collapsed.\"\n",
    "        \n",
    "    # Test resolution collapsing.\n",
    "    assert (\"resolution\", \"~2600x1400\") in collapsed_data,\\\n",
    "           \"Collapsed resolutions with enough elements must be reported, prefixed with ~.\"\n",
    "    assert (\"resolution\", \"Other\") in collapsed_data,\\\n",
    "           \"Collapsed resolutions with not enough elements must be reported as 'Other'.\"\n",
    "    assert collapsed_data[(\"resolution\", \"Other\")] == 23,\\\n",
    "           \"The 640x* and the 0x0 resolution must be reported in the 'Other' bucket.\"\n",
    "        \n",
    "    # Test that whitelisted keys are not collapsed.\n",
    "    assert (\"has_flash\", False) in collapsed_data,\\\n",
    "           \"Whitelisted keys must not be collapsed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the test coverage for finalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_finalize_data():\n",
    "    collapsed_data = {\n",
    "        ('os', 'Darwin-11.0') : 22,\n",
    "        ('resolution', '1280x1024') : 50,\n",
    "        ('os', 'Windows_NT-6.11') : 34,\n",
    "        ('resolution', '~2600x1400') : 16,\n",
    "        ('os', 'Other') : 2,\n",
    "        ('has_flash', True) : 72,\n",
    "        ('os', 'Windows_NT-Other') : 16,\n",
    "        ('has_flash', False) : 2,\n",
    "        ('resolution', 'Other') : 8\n",
    "    }\n",
    "\n",
    "    finalized_data = finalize_data(collapsed_data, 74, 0.1, 0.2, dt.date(2016, 7, 3))\n",
    "    \n",
    "    # Check that the basic fields are in the finalized data.\n",
    "    assert finalized_data['broken'] == 0.1,\\\n",
    "           \"The ratio of broken data must be correctly reported.\"\n",
    "    assert finalized_data['inactive'] == 0.2,\\\n",
    "           \"The ratio of inactive clients must be correctly reported.\"\n",
    "    assert finalized_data['date'] == \"2016-07-03\",\\\n",
    "           \"The first day of the reporting period must be reported.\"\n",
    "        \n",
    "    # Make sure that all the reported numbers are ratios.\n",
    "    all_ratios = [(v >= 0.0 and v <= 1.0) for (k, v) in finalized_data.iteritems() if k != 'date']\n",
    "    assert all(all_ratios),\\\n",
    "           \"All the reported entries must be ratios.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test coverage for the aggregated data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_validate_finalized_data():\n",
    "    MISSING_KEYS = {\n",
    "        'browserArch_x86': 1.0,\n",
    "        'cpuCores_2': 1.0\n",
    "    }\n",
    "\n",
    "    assert validate_finalized_data(MISSING_KEYS) == False,\\\n",
    "           \"The validator must fail when expected keys are missing\"\n",
    "        \n",
    "    KEYS_NOT_ADDING_UP = {\n",
    "        'browserArch_x86': 0.5,\n",
    "        'browserArch_x64': 0.4,\n",
    "        'cpuCores_1': 1.0,\n",
    "        'cpuCoresSpeed_2_2.2': 1.0,\n",
    "        'cpuVendor_Vendor1': 1.0,\n",
    "        'cpuSpeed_2.2': 1.0,\n",
    "        'gpuVendor_Vendor3': 1.0,\n",
    "        'gpuModel_Model1': 1.0,\n",
    "        'resolution_800x600': 1.0,\n",
    "        'ram_2': 1.0,\n",
    "        'osName_SomeOS': 1.0,\n",
    "        'osArch_x64': 1.0,\n",
    "        'hasFlash_True': 1.0\n",
    "    }\n",
    "\n",
    "    assert validate_finalized_data(KEYS_NOT_ADDING_UP) == False,\\\n",
    "           \"The validator must fail when the reported values don't add up to 1.0\"\n",
    "    \n",
    "    WORKING_DATA = {\n",
    "        'browserArch_x86': 0.7,\n",
    "        'browserArch_x64': 0.29,\n",
    "        'cpuCores_1': 0.5,\n",
    "        'cpuCores_2': 0.5,\n",
    "        'cpuCoresSpeed_2_2.2': 0.1,\n",
    "        'cpuCoresSpeed_2_2.4': 0.9,\n",
    "        'cpuVendor_Vendor1': 0.725,\n",
    "        'cpuVendor_Vendor2': 0.275,\n",
    "        'cpuSpeed_2.2': 0.1,\n",
    "        'cpuSpeed_2.4': 0.9,\n",
    "        'gpuVendor_Vendor3': 0.9,\n",
    "        'gpuVendor_Vendor4': 0.1,\n",
    "        'gpuModel_Model1': 0.00001,\n",
    "        'gpuModel_Model2': 0.99999,\n",
    "        'resolution_800x600': 1.0,\n",
    "        'ram_2': 1.0,\n",
    "        'osName_SomeOS': 1.0,\n",
    "        'osArch_x64': 1.0,\n",
    "        'hasFlash_True': 1.0,\n",
    "        'broken': 0.1,\n",
    "        'inactive': 0.1,\n",
    "        'date': '2017-03-26'\n",
    "    }\n",
    "\n",
    "    assert validate_finalized_data(WORKING_DATA),\\\n",
    "           \"The validator must not fail when the reported data is correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    # Does |get_OS_arch| work as expected?\n",
    "    assert get_OS_arch(\"x86\", \"Windows_NT\", False) == \"x86\",\\\n",
    "           \"get_OS_arch should report an 'x86' for an x86 browser with no is_wow64.\"\n",
    "    assert get_OS_arch(\"x86\", \"Windows_NT\", True) == \"x86-64\",\\\n",
    "           \"get_OS_arch should report an 'x86-64' for an x86 browser, on Windows, using Wow64.\"\n",
    "    assert get_OS_arch(\"x86\", \"Darwin\", True) == \"x86\",\\\n",
    "           \"get_OS_arch should report an 'x86' for an x86 browser on non Windows platforms.\"\n",
    "    assert get_OS_arch(\"x86-64\", \"Darwin\", True) == \"x86-64\",\\\n",
    "           \"get_OS_arch should report an 'x86-64' for an x86-64 browser on non Windows platforms.\"\n",
    "    assert get_OS_arch(\"x86-64\", \"Windows_NT\", False) == \"x86-64\",\\\n",
    "           \"get_OS_arch should report an 'x86-64' for an x86-64 browser on Windows platforms.\"\n",
    "        \n",
    "    # Does |vendor_name_from_id| behave correctly?\n",
    "    assert vendor_name_from_id(\"0x1013\") == \"Cirrus Logic\",\\\n",
    "           \"vendor_name_from_id must report the correct vendor name for a known vendor id.\"\n",
    "    assert vendor_name_from_id(\"0xfeee\") == \"Other\",\\\n",
    "           \"vendor_name_from_id must report 'Other' for an unknown vendor id.\"\n",
    "   \n",
    "    # Make sure |invert_device_map| works as expected.\n",
    "    device_data = {\"feee\": {\"family\":{\"chipset\":[\"d1d1\", \"d2d2\"]}}}\n",
    "    inverted_device_data = invert_device_map(device_data)\n",
    "    assert \"0xfeee\" in inverted_device_data,\\\n",
    "           \"The vendor id must be prefixed with '0x' and be at the root of the map.\"\n",
    "    assert len(inverted_device_data[\"0xfeee\"].keys()) == 2,\\\n",
    "           \"There must be two devices for the '0xfeee' vendor.\"\n",
    "    assert all(device_id in inverted_device_data[\"0xfeee\"] for device_id in (\"0xd1d1\", \"0xd2d2\")),\\\n",
    "           \"The '0xfeee' vendor must contain the expected devices.\"\n",
    "    assert all(d in inverted_device_data[\"0xfeee\"][\"0xd1d1\"] for d in (\"family\", \"chipset\")),\\\n",
    "           \"The family and chipset data must be reported in the device section.\"\n",
    "    \n",
    "    # Let's test |get_device_family_chipset|.\n",
    "    global device_map\n",
    "    device_map = inverted_device_data\n",
    "    assert get_device_family_chipset(\"0xfeee\", \"0xd1d1\") == \"family-chipset\",\\\n",
    "           \"The family and chipset info must be returned as '<family>-<chipset>' for known devices.\"\n",
    "    assert get_device_family_chipset(\"0xfeee\", \"0xdeee\") == \"Unknown\",\\\n",
    "           \"Unknown devices must be reported as 'Unknown'.\"\n",
    "    assert get_device_family_chipset(\"0xfeeb\", \"0xdeee\") == \"Unknown\",\\\n",
    "           \"Unknown families must be reported as 'Unknown'.\"\n",
    "    \n",
    "    # Test |prepare_data|.\n",
    "    test_prepare_data()\n",
    "    \n",
    "    # Test |aggregate_data|.\n",
    "    test_aggregate_data()\n",
    "    \n",
    "    # Test |collapse_buckets|.\n",
    "    test_collapse_buckets()\n",
    "    \n",
    "    # Test |finalize_data|.\n",
    "    test_finalize_data()\n",
    "    \n",
    "    # Test |validate_finalized_data|\n",
    "    test_validate_finalized_data()\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
